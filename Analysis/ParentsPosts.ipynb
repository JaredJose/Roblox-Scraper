{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#import pyLDAvis.gensim\n",
    "import gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import os\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa314e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/robloxParentsPosts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a018a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c11d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].str.count('game').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d75b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(df['text'].dropna().astype(str))\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.strip() != '']\n",
    "\n",
    "\n",
    "bi_grams = ngrams(filtered_tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list = list(bi_grams)\n",
    "bi_grams_list = [tuple(filter(None, bg)) for bg in bi_grams_list]\n",
    "#print(bi_grams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list1 = [' '.join(bi_gram) for bi_gram in bi_grams_list if '(' not in bi_gram and ')' not in bi_gram]\n",
    "\n",
    "freq_dist = nltk.FreqDist(bi_grams_list1)\n",
    "most_common = freq_dist.most_common(1)\n",
    "print('The most common bi-gram is:', most_common[0][0], 'with a frequency of', most_common[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_na = df.dropna(subset=['text'])\n",
    "df_ques = df_no_na['text'].str.endswith('?')\n",
    "df_true = df_no_na.loc[df_ques]\n",
    "print(df_true['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09571993",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
    "X1=vectorizer.fit_transform(bi_grams_list1)\n",
    "features = (vectorizer.get_feature_names_out())\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "tfidf = vectorizer.fit_transform(bi_grams_list1)\n",
    "\n",
    "\n",
    "sums = tfidf.sum(axis = 0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append( (term, sums[0, col] ))\n",
    "ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "words = (ranking.sort_values('rank', ascending = False))\n",
    "print (\"\\n\\nWords : \\n\", words.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9e371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text'].apply(lambda x: re.sub('[,.!?]', '', x) if isinstance(x, str) else x)\n",
    "df['text_processed'] = df['text_processed'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "df['text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48404af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5bf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "num_topics = 10\n",
    "filename = 'ParentsPosts'\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/'+filename)\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "#s.makedirs('./results/ldavis_prepared_10')\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/'+ filename +'.html')\n",
    "LDAvis_prepared\n",
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a48751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba166f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7ab36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b093a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
