{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "#import pyLDAvis.gensim\n",
    "import gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import os\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31443236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ALLrobloxParents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e845c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623969e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(df['text'].dropna().astype(str))\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.strip() != '']\n",
    "\n",
    "\n",
    "bi_grams = ngrams(filtered_tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list = list(bi_grams)\n",
    "bi_grams_list = [tuple(filter(None, bg)) for bg in bi_grams_list]\n",
    "#print(bi_grams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list1 = [' '.join(bi_gram) for bi_gram in bi_grams_list if '(' not in bi_gram and ')' not in bi_gram]\n",
    "\n",
    "freq_dist = nltk.FreqDist(bi_grams_list1)\n",
    "most_common = freq_dist.most_common(1)\n",
    "print('The most common bi-gram is:', most_common[0][0], 'with a frequency of', most_common[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad3957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bi_grams_list1 = [' '.join(bi_gram) for bi_gram in bi_grams_list if '(' not in bi_gram and ')' not in bi_gram]\n",
    "\n",
    "if bi_grams_list1:\n",
    "    freq_dist = nltk.FreqDist(bi_grams_list1)\n",
    "    most_common = freq_dist.most_common(1)\n",
    "    print('The most common bi-gram is:', most_common[0][0], 'with a frequency of', most_common[0][1])\n",
    "else:\n",
    "    print('No bigrams found in the text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "words = [word for word in words if word.lower() not in stop_words]\n",
    "word_counts = Counter(words)\n",
    "\n",
    "for word, count in word_counts.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4597d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
    "X1=vectorizer.fit_transform(bi_grams_list1)\n",
    "features = (vectorizer.get_feature_names_out())\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "tfidf = vectorizer.fit_transform(bi_grams_list1)\n",
    "\n",
    "\n",
    "sums = tfidf.sum(axis = 0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append( (term, sums[0, col] ))\n",
    "ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "words = (ranking.sort_values('rank', ascending = False))\n",
    "print (\"\\n\\nWords : \\n\", words.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad991e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ques = df['text'].str.endswith('?')\n",
    "df_true = df.loc[df_ques]\n",
    "print(df_true['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text'].apply(lambda x: re.sub('[,.!?]', '', x) if isinstance(x, str) else x)\n",
    "df['text_processed'] = df['text_processed'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "df['text_processed'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a642f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new file ParentsComments.html and result10\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "num_topics = 10\n",
    "filename = 'AllRobloxParents'\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/'+filename)\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared,  './results/' + filename +'.html')\n",
    "LDAvis_prepared\n",
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f986e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
