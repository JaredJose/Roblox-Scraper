{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#import pyLDAvis.gensim\n",
    "import gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/filteredParentingPosts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c310e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].str.count('game').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(df['text'].dropna().astype(str))\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.strip() != '']\n",
    "\n",
    "\n",
    "bi_grams = ngrams(filtered_tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list = list(bi_grams)\n",
    "bi_grams_list = [tuple(filter(None, bg)) for bg in bi_grams_list]\n",
    "#print(bi_grams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams_list1 = [' '.join(bi_gram) for bi_gram in bi_grams_list if '(' not in bi_gram and ')' not in bi_gram]\n",
    "\n",
    "freq_dist = nltk.FreqDist(bi_grams_list1)\n",
    "most_common = freq_dist.most_common(1)\n",
    "print('The most common bi-gram is:', most_common[0][0], 'with a frequency of', most_common[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (2, 2))\n",
    "X1=vectorizer.fit_transform(bi_grams_list1)\n",
    "features = (vectorizer.get_feature_names_out())\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "tfidf = vectorizer.fit_transform(bi_grams_list1)\n",
    "\n",
    "\n",
    "sums = tfidf.sum(axis = 0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append( (term, sums[0, col] ))\n",
    "ranking = pd.DataFrame(data1, columns = ['term', 'rank'])\n",
    "words = (ranking.sort_values('rank', ascending = False))\n",
    "print (\"\\n\\nWords : \\n\", words.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ques = df['text'].str.endswith('?')\n",
    "df_no_na = df.dropna(subset=['text'])\n",
    "df_true = df_no_na.loc[df_ques]\n",
    "print(df_true['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07aa218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all commas...\n",
    "df['text_processed'] = df['text'].apply(lambda x: re.sub('[,.!?]', '', x) if isinstance(x, str) else x)\n",
    "df['text_processed'] = df['text_processed'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "df['text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33891737",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "filename = 'filteredParentingPosts'\n",
    "num_topics = 10\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/'+filename)\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "#s.makedirs('./results/ldavis_prepared_10')\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/'+ filename +'.html')\n",
    "LDAvis_prepared\n",
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1111e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946c992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54fa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
